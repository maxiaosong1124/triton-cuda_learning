# Triton ç®—å­å¼€å‘: è°ƒè¯•ä¸æ€§èƒ½ä¼˜åŒ–å®æˆ˜

ä½œè€…ï¼š1nfinite æ— é™æ™ºåŸŸ

## å¼•è¨€

Triton ä½œä¸ºé«˜æ€§èƒ½ AI ç®—å­å¼€å‘æ¡†æ¶ï¼Œæ”¯æŒå¼€å‘è€…å¿«é€Ÿå®ç°é«˜æ•ˆçš„ GPU ç®—å­ã€‚ä½†ç®—å­å¼€å‘è¿‡ç¨‹ä¸­ï¼Œå¸¸é¢ä¸´é€»è¾‘é”™è¯¯ã€æ˜¾å­˜è®¿é—®å¼‚å¸¸ã€æ€§èƒ½ä¸è¾¾é¢„æœŸç­‰é—®é¢˜ã€‚æœ¬æ–‡å°†ä»è°ƒè¯•å·¥å…·ã€è‡ªåŠ¨ä¼˜åŒ–æœºåˆ¶ã€æ€§èƒ½åˆ†æå·¥å…·ä¸‰ä¸ªç»´åº¦ï¼Œç»“åˆå®æˆ˜æ¡ˆä¾‹ï¼Œè¯¦ç»†è®²è§£ Triton ç®—å­çš„è°ƒè¯•ä¸æ€§èƒ½ä¼˜åŒ–æ–¹æ³•ï¼Œå¸®åŠ©å¼€å‘è€…æå‡å¼€å‘æ•ˆç‡ä¸ç®—å­æ€§èƒ½ã€‚

## Triton è°ƒè¯•

Triton æä¾›äº†å¤šç§è°ƒè¯•å·¥å…·ï¼Œæ¶µç›–ç¼–è¯‘æ—¶æ£€æŸ¥ã€è¿è¡Œæ—¶æ‰“å°ã€CPU è§£ææ‰§è¡Œç­‰åœºæ™¯ï¼Œå¸®åŠ©å¼€å‘è€…ç²¾å‡†å®šä½é”™è¯¯ã€‚

### Debugging Ops

Triton å†…ç½® 4 ç±»è°ƒè¯•ç®—å­ï¼Œæ”¯æŒç¼–è¯‘æ—¶ä¸è¿è¡Œæ—¶çš„æ•°å€¼æ£€æŸ¥ã€æ–­è¨€éªŒè¯ï¼Œé€‚ç”¨äºå¿«é€Ÿæ’æŸ¥æ•°æ®å¼‚å¸¸ã€æ˜¾å­˜è¶Šç•Œç­‰é—®é¢˜ã€‚è°ƒè¯•ç®—å­åŠŸèƒ½åŠæ‰§è¡Œæ–¹æ³•å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š

| è°ƒè¯•ç®—å­ | åŠŸèƒ½æè¿° | æ‰§è¡Œæ¡ä»¶ |
| --- | --- | --- |
| static_print | ç¼–è¯‘æ—¶æ‰“å°å€¼ï¼ˆå¦‚å¸¸é‡ã€é…ç½®å‚æ•°ï¼‰ | ä¸å— TRITON_DEBUG ç¯å¢ƒå˜é‡å½±å“ |
| static_assert | ç¼–è¯‘æ—¶æ–­è¨€æ¡ä»¶ï¼ˆå¦‚å‚æ•°åˆæ³•æ€§æ ¡éªŒï¼‰ | ä¸æ»¡è¶³æ¡ä»¶åˆ™ç¼–è¯‘å¤±è´¥ |
| device_print | è¿è¡Œæ—¶ä» GPU è®¾å¤‡æ‰“å°å˜é‡å€¼ï¼ˆå¦‚çº¿ç¨‹ IDã€æ•°æ®ï¼‰ | ä¸å— TRITON_DEBUG ç¯å¢ƒå˜é‡å½±å“ |
| device_assert | è¿è¡Œæ—¶æ–­è¨€æ¡ä»¶ï¼ˆå¦‚æ•°æ®èŒƒå›´æ ¡éªŒï¼‰ | ä»…å½“ TRITON_DEBUG=1 æ—¶æ‰§è¡Œ |

- æ¡ˆä¾‹ï¼šadd_kernel è°ƒè¯•

åœ¨å‘é‡åŠ æ³•ç®—å­ä¸­ï¼Œé€šè¿‡`static_print`æ‰“å°ç¼–è¯‘æ—¶çš„`BLOCK_SIZE`ï¼Œé€šè¿‡`device_print`æ‰“å°è¿è¡Œæ—¶çš„çº¿ç¨‹ IDï¼ˆpidï¼‰ï¼š

```
import torch

import triton
import triton.language as tl


@triton.jit
def add_kernel(x_ptr,  # *Pointer* to first input vector.
               y_ptr,  # *Pointer* to second input vector.
               output_ptr,  # *Pointer* to output vector.
               n_elements,  # Size of the vector.
               BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.
               # NOTE: `constexpr` so it can be used as a shape value.
               ):
    pid = tl.program_id(axis=0)
    tl.static_print(f"BLOCK_SIZE:{BLOCK_SIZE}")
    if pid == 1:
        tl.device_print("pid",pid)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    tl.store(output_ptr + offsets, output, mask=mask)
```

- å¸¸è§åº”ç”¨åœºæ™¯

### interpreter æ¨¡å¼ï¼ˆcpu python è§£ææ‰§è¡Œï¼‰

Triton çš„`interpreter`æ¨¡å¼å¯å°†ç®—å­ä»£ç è½¬æ¢ä¸º Python ä»£ç åœ¨ CPU ä¸Šæ‰§è¡Œï¼Œæ— éœ€ç¼–è¯‘ä¸º GPU æŒ‡ä»¤ï¼Œæ”¯æŒ Python è°ƒè¯•ç”Ÿæ€ï¼ˆå¦‚ pdbï¼‰ï¼Œæ˜¯æ’æŸ¥é€»è¾‘é”™è¯¯ã€è¯­æ³•é”™è¯¯çš„é«˜æ•ˆå·¥å…·ã€‚

å¯ç”¨æ–¹å¼ï¼ˆäºŒé€‰ä¸€ï¼‰

```
import triton

# æ–¹æ³•1ï¼šç¯å¢ƒå˜é‡
import os
os.environ['TRITON_INTERPRET'] = '1'

# æ–¹æ³•2ï¼šåœ¨ä»£ç ä¸­è®¾ç½®
triton.runtime.driver.active.set_interpret_mode(True)
```

- æ¡ˆä¾‹ï¼šç»“åˆ pdb è°ƒè¯• add_kernel

åœ¨ç®—å­ä»£ç ä¸­æ’å…¥ pdb æ–­ç‚¹ï¼Œå¯ç”¨ interpreter æ¨¡å¼åï¼Œå¯é€æ­¥æ‰§è¡Œä»£ç å¹¶è§‚å¯Ÿå˜é‡å˜åŒ–ï¼Œå…·ä½“å®ç°å¦‚ä¸‹ï¼š

```
@triton.jit
def add_kernel(
    x_ptr, y_ptr, output_ptr, n_elements,
    BLOCK_SIZE: tl.constexpr
):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets
    x = tl.load(x_ptr + offsets, mask=mask)

    # æ’å…¥pdbæ–­ç‚¹ï¼ˆä»…åœ¨interpretæ¨¡å¼ä¸‹ç”Ÿæ•ˆï¼‰
    import pdb; pdb.set_trace()

    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    tl.store(output_ptr + offsets, output, mask=mask)

# è¿è¡Œå‘½ä»¤ï¼ˆå¯ç”¨interpretæ¨¡å¼ï¼‰
# TRITON_INTERPRET=1 python add_kernel.py
```

å¦‚å›¾ä¸­æ‰€ç¤ºï¼Œè¿è¡Œåå°†è‡ªåŠ¨è¿›å…¥ pdb äº¤äº’ç¯å¢ƒï¼Œå¯é€šè¿‡ä»¥ä¸‹æŒ‡ä»¤è¿›è¡Œæ–­ç‚¹è°ƒè¯•ï¼š

1. è¾“å…¥ `n`ï¼ˆnextï¼‰ï¼šå•æ­¥æ‰§è¡Œä»£ç ï¼Œé€æ­¥æ¨è¿›æ‰§è¡Œæµç¨‹ï¼›
2. è¾“å…¥ `print(offsets)`ã€`print(mask)` ç­‰ï¼šæŸ¥çœ‹æŒ‡å®šå˜é‡çš„å€¼ï¼ŒéªŒè¯é€»è¾‘æ­£ç¡®æ€§ï¼›
3. è¾“å…¥ `c`ï¼ˆcontinueï¼‰ï¼šç»§ç»­æ‰§è¡Œåˆ°ä¸‹ä¸€ä¸ªæ–­ç‚¹ï¼ˆè‹¥å­˜åœ¨ï¼‰ã€‚

- æŠ€å·§
- é™åˆ¶ä¸é€‚ç”¨åœºæ™¯

âœ… ä¼˜ç‚¹ï¼š

- å¿«é€Ÿè¿­ä»£ï¼šæ— éœ€ç¼–è¯‘ï¼Œç¼©çŸ­è°ƒè¯•å‘¨æœŸï¼›
- é€»è¾‘å®šä½ï¼šç²¾å‡†æ’æŸ¥å¾ªç¯è¾¹ç•Œã€æ¡ä»¶åˆ¤æ–­ç­‰é€»è¾‘é”™è¯¯ï¼›
- è·¨å¹³å°å…¼å®¹ï¼šæ—  GPU ç¯å¢ƒä¹Ÿå¯è°ƒè¯•ï¼›
- æ•™å­¦å‹å¥½ï¼šé€æ­¥è§‚å¯Ÿç®—å­æ‰§è¡Œæµç¨‹ã€‚

âš ï¸ å±€é™æ€§ï¼š

- ä¸æ”¯æŒ bfloat16 ç±»å‹ï¼ˆéœ€é€šè¿‡`tl.cast`è½¬ä¸º float32ï¼‰ï¼›
- ä¸æ”¯æŒé—´æ¥å†…å­˜è®¿é—®ï¼ˆå¦‚`ptr = tl.load(ptr); x = tl.load(ptr)`ï¼‰ï¼›
- æ€§èƒ½è¾ƒå·®ï¼šè§£é‡Šæ‰§è¡Œæ¯” GPU ç¼–è¯‘æ‰§è¡Œæ…¢ 10~100 å€ã€‚

### ä¸‰æ–¹è°ƒè¯•å·¥å…·

é™¤å†…ç½®å·¥å…·å¤–ï¼ŒTriton æ”¯æŒé›†æˆç¬¬ä¸‰æ–¹å·¥å…·ï¼Œæ’æŸ¥å¤æ‚é”™è¯¯ï¼ˆå¦‚æ•°æ®ç«äº‰ã€å†…å­˜æ³„æ¼ï¼‰ï¼š

| å·¥å…·åç§° | é€‚ç”¨åœºæ™¯ | ä½¿ç”¨æ–¹å¼ |
| --- | --- | --- |
| compute-sanitizer | NVIDIA GPUï¼šæ•°æ®ç«äº‰ã€å†…å­˜è®¿é—®å¼‚å¸¸ | å‘½ä»¤å‰ç¼€ï¼šcompute-sanitizer python xxx.py |
| LLVM AddressSanitizer | AMD GPUï¼šå†…å­˜è¶Šç•Œã€ä½¿ç”¨åé‡Šæ”¾ | ç¼–è¯‘æ—¶å¯ç”¨ ROCm sanitizer æ’ä»¶ |
| triton-viz | å†…å­˜è®¿é—®å¯è§†åŒ–ï¼ˆè·¨ GPU æ¶æ„ï¼‰ | å®‰è£…åç”Ÿæˆå†…å­˜è®¿é—®çƒ­åŠ›å›¾ |
| TritonSanï¼ˆTritonSanitizerï¼‰ | CPU åç«¯ï¼šå†…å­˜è¶Šç•Œã€æ•°æ®ç«äº‰ã€æœªåˆå§‹åŒ–å˜é‡ | è‡ªåŠ¨æ’æ¡© LLVM æ£€æµ‹å·¥å…·ï¼Œç¼–è¯‘ä¸º CPU å¯æ‰§è¡Œæ–‡ä»¶ |

- TritonSan è§£æ

TritonSan æ˜¯ Triton æ¡†æ¶ä¸“é—¨é’ˆå¯¹ CPU åç«¯çš„é”™è¯¯æ£€æµ‹å·¥å…·ï¼Œä¾æ‰˜ triton-sharedï¼ˆTriton ç¼–è¯‘å™¨çš„å…±äº«ä¸­é—´ä»¶å±‚ï¼‰å°† Triton å†…æ ¸ç¼–è¯‘ä¸º CPU å¯æ‰§è¡Œæ–‡ä»¶ã€‚åœ¨ç¼–è¯‘è¿‡ç¨‹ä¸­ï¼ŒTritonSan ä¼šå¯ç”¨ LLVM æ£€æµ‹å·¥å…·ï¼ˆsanitizersï¼‰çš„æ’æ¡©æœºåˆ¶ï¼Œå¹¶å®æ–½å¿…è¦çš„ä»£ç è½¬æ¢ï¼Œç¡®ä¿æ£€æµ‹å·¥å…·èƒ½è·å–å®Œæ•´çš„è°ƒè¯•ä¿¡æ¯ï¼›å½“ Triton å†…æ ¸æ‰§è¡Œæ—¶ï¼Œå°†ä¸æŒ‡å®šçš„ LLVM æ£€æµ‹å·¥å…·ååŒè¿è¡Œï¼Œå®ç°å¯¹å†…æ ¸å†…éƒ¨é”™è¯¯çš„ç²¾å‡†æ£€æµ‹ã€‚

TritonSan æ¶µç›–é™æ€æ£€æµ‹ä¸åŠ¨æ€æ£€æµ‹ä¸¤ç§æ¨¡å¼ï¼šé™æ€æ£€æµ‹å¯åœ¨ç®—å­ç¼–è¯‘é˜¶æ®µåˆ†æä»£ç ï¼Œå‘ç°æ½œåœ¨çš„è¯­æ³•é”™è¯¯ã€é€»è¾‘é£é™©ï¼›åŠ¨æ€æ£€æµ‹åˆ™åœ¨ç®—å­æ‰§è¡Œè¿‡ç¨‹ä¸­å®æ—¶ç›‘æ§å†…å­˜è®¿é—®ã€çº¿ç¨‹è¡Œä¸ºï¼Œä¸€æ—¦è§¦å‘é”™è¯¯ä¾¿ç«‹å³è¾“å‡ºè¯¦ç»†æç¤ºï¼Œå¤§å¹…æå‡å¤æ‚é”™è¯¯çš„æ’æŸ¥æ•ˆç‡ã€‚ä¸‹å›¾åˆ™æ˜¯ tritonsan å·¥å…·çš„ä¸€ä¸ªå®ç°åŸç†ï¼š

- TritonSan åŸºæœ¬å¯ç”¨æ–¹æ³•

```
# æŸ¥çœ‹ä½¿ç”¨è¯´æ˜
triton-san <sanitizer type> <original command used to launch the triton program...>

# sanitizer type å¯é€‰å€¼ï¼š
# "asan": æ£€æµ‹ç¼“å†²åŒºæº¢å‡º
# "tsan": æ£€æµ‹æ•°æ®ç«äº‰

# ç¤ºä¾‹ï¼šä½¿ç”¨ asan æ£€æµ‹ç®—å­å†…å­˜è¶Šç•Œé—®é¢˜
triton-san asan python ./my_triton_program.py
from triton.backends.triton_shared.driver import CPUDriver
triton.runtime.driver.set_active(CPUDriver())

# ç¡®ä¿è¾“å‡ºå¼ é‡åœ¨ CPU ä¸Šï¼ˆé€‚é… CPU åç«¯æ£€æµ‹ï¼‰
# output = torch.empty((size, )).to("gpu")  # æ³¨é‡Šæ‰ GPU è¾“å‡º
output = torch.empty((size, )).to("cpu")  # æ”¹ä¸º CPU è¾“å‡º
```

## Triton è°ƒä¼˜

Triton é‡‡ç”¨ SPMDï¼ˆå•ç¨‹åºå¤šæ•°æ®ï¼‰æ¨¡å‹å®ç°é«˜æ€§èƒ½å¹¶è¡Œï¼Œå…¶æ ¸å¿ƒé€»è¾‘ä¸ºï¼šåœ¨å¤šæ ¸ GPU æ¶æ„ä¸‹ï¼Œä¸åŒ PEï¼ˆå¤„ç†å•å…ƒï¼‰æ‰§è¡Œç›¸åŒç¨‹åºï¼Œä½†å„è‡ªå¤„ç†ä¸åŒæ•°æ®ï¼›æ¯ä¸ª PE ç”±å¤šçº¿ç¨‹å®ç°å¹¶å‘æ‰§è¡Œï¼Œæ•°æ®åŠ è½½é€Ÿåº¦ä¸»è¦å–å†³äº PE åˆ° DRAM çš„å¸¦å®½ï¼Œä»¥åŠ GPU è‡ªèº«çš„å†…å­˜è®¿é—®æ¨¡å¼ã€‚

åŸºäºä¸Šè¿°æ¶æ„ç‰¹æ€§ï¼ŒTriton ç®—å­çš„æ€§èƒ½ä¸å¤šä¸ªå…³é”®å› ç´ å¯†åˆ‡ç›¸å…³ï¼Œå…¶ä¸­æ ¸å¿ƒå½±å“å› ç´ åŒ…æ‹¬ block sizeã€num_warps ç­‰é…ç½®å‚æ•°ï¼Œä»¥åŠ GPU å†…å­˜è®¿é—®æ¨¡å¼ã€‚ä¸åŒå‚æ•°é…ç½®é€‚é…ä¸åŒçš„æ•°æ®é‡åœºæ™¯ï¼Œå¦‚ï¼š

- å¤§æ•°æ®é‡åœºæ™¯ï¼šblock size é…ç½®è¶‹äºå¸¦å®½å¤§å°æ›´ä¼˜ï¼Œå¯æœ‰æ•ˆå‡å°‘æ•°æ®å¯»å€è¿‡ç¨‹ä¸­çš„å¼€é”€ï¼Œæå‡æ•°æ®è¯»å–ä¸å¤„ç†æ•ˆç‡ï¼›
- å°æ•°æ®é‡åœºæ™¯ï¼šblock size é…ç½®è¶‹äºæ•°æ®å¤§å°æ›´ä¼˜ï¼Œèƒ½å¤Ÿé¿å…å› æ•°æ®å¡«å……å¯¼è‡´çš„è®¡ç®—èµ„æºæµªè´¹ï¼Œé™ä½æ•ˆç‡ä¸‹é™é£é™©ã€‚

åœ¨å®é™…ä¼˜åŒ–ä¸­ï¼Œä¸Šè¿° block sizeã€num_warps ç­‰å…³é”®å‚æ•°ï¼Œé€šå¸¸ä¼šè¢«è®¾ç½®ä¸ºç¼–è¯‘æ—¶çš„å¸¸é‡ï¼Œä»¥æ­¤å®ç°é’ˆå¯¹æ€§çš„æ€§èƒ½ä¼˜åŒ–ã€‚

### è‡ªåŠ¨ä¼˜åŒ–æœºåˆ¶

ä¸ºç®€åŒ–å‚æ•°é…ç½®æµç¨‹ã€æå‡é€‚é…æ€§ï¼ŒTriton å†…ç½®äº†ä¸¤ç§æ ¸å¿ƒè‡ªåŠ¨ä¼˜åŒ–æœºåˆ¶â€”â€”Autotuneï¼ˆè‡ªåŠ¨å‚æ•°æœç´¢ä¸å¯å‘å¼ä¼˜åŒ–ã€‚è¿™ä¸¤ç§æœºåˆ¶å¯è‡ªåŠ¨æ§åˆ¶ block sizeã€num_warps ç­‰å…³é”®å‚æ•°çš„è®¾ç½®ï¼Œæ ¹æ®å…·ä½“ä¸šåŠ¡åœºæ™¯ï¼ˆåŒ…æ‹¬ç¡¬ä»¶å‹å·ã€æ•°æ®è§„æ¨¡ã€è®¡ç®—ä»»åŠ¡ç±»å‹ç­‰ï¼‰åŠ¨æ€è°ƒæ•´å‚æ•°å…ƒæ•°æ®ï¼ˆmetadataï¼‰ï¼Œä»è€Œå®ç°å¯¹ä¸åŒç¡¬ä»¶ç¯å¢ƒä¸æ•°æ®è§„æ¨¡çš„ç²¾å‡†é€‚é…ï¼Œæ— éœ€äººå·¥æ‰‹åŠ¨è°ƒè¯•å³å¯è¾¾åˆ°è¾ƒä¼˜æ€§èƒ½ã€‚

#### Autotuneï¼ˆè‡ªåŠ¨å‚æ•°æœç´¢ï¼‰

Autotune æ˜¯ Triton ä¸­æ ¸å¿ƒçš„è‡ªåŠ¨ä¼˜åŒ–æ‰‹æ®µï¼Œå…¶æ ¸å¿ƒæ€è·¯æ˜¯é€šè¿‡éå†é¢„è®¾çš„å‚æ•°ç©ºé—´ï¼Œè‡ªåŠ¨ç­›é€‰å‡ºæœ€ä¼˜çš„å‚æ•°ç»„åˆï¼ˆå¦‚ block size ä¸ num_warps çš„æ­é…ï¼‰ï¼Œå…·å¤‡æå¼ºçš„åœºæ™¯é€‚é…æ€§ï¼Œå¯å¹¿æ³›é€‚ç”¨äºä¸åŒç¡¬ä»¶å‹å·ã€ä¸åŒè¾“å…¥æ•°æ®è§„æ¨¡çš„å„ç±»åœºæ™¯ã€‚

- æ ¸å¿ƒä¾èµ–
- å·¥ä½œåŸç†
- æ¡ˆä¾‹ï¼šAutotune ä¼˜åŒ– matmul ç®—å­

```
import triton
import triton.language as tl

# å®šä¹‰å€™é€‰é…ç½®ï¼ˆä¸åŒ block size ä¸ num_warps ç»„åˆï¼‰
configs = [
    triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_warps=4),
    triton.Config({'BLOCK_M': 256, 'BLOCK_N': 256, 'BLOCK_K': 32}, num_warps=8),
    triton.Config({'BLOCK_M': 512, 'BLOCK_N': 512, 'BLOCK_K': 64}, num_warps=16),
]

# ä»¥è¾“å…¥çŸ©é˜µç»´åº¦ï¼ˆMã€Nã€Kï¼‰ä¸ºè°ƒä¼˜keyï¼Œé€‚é…ä¸åŒè¾“å…¥è§„æ¨¡
@triton.autotune(configs=configs, key=['M', 'N', 'K'])
@triton.jit
def matmul_kernel(
    a_ptr, b_ptr, c_ptr,
    M, N, K,  # çŸ©é˜µç»´åº¦
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    # çŸ©é˜µä¹˜æ³•æ ¸å¿ƒé€»è¾‘ï¼ˆçœç•¥ï¼Œæ ¸å¿ƒä¸ºåˆ†å—çŸ©é˜µä¹˜æ³•ä¸å†…å­˜é«˜æ•ˆè®¿é—®ï¼‰
    pass
```

#### å¯å‘å¼ä¼˜åŒ–

Triton é€šè¿‡`@triton.heuristics`è£…é¥°å™¨é¢„å®šä¹‰å‚æ•°è§„åˆ™ï¼Œç»“åˆå½“å‰ç¡¬ä»¶ç‰¹æ€§ä¸è¾“å…¥æ•°æ®ç‰¹å¾ï¼Œç›´æ¥æ¨å¯¼å¹¶ç”Ÿæˆæœ€ä¼˜å‚æ•°é…ç½®ï¼Œæ— éœ€éå†å…¨éƒ¨å‚æ•°ç©ºé—´ï¼Œä»è€Œå®ç°â€œå¿«é€Ÿå†³ç­–ã€å³æ—¶é€‚é…â€çš„ä¼˜åŒ–æ•ˆæœã€‚

- å¯å‘å¼ä¼˜åŒ–çš„å‚æ•°å†³ç­–ä¾èµ– Triton å†…ç½®çš„ç»éªŒè§„åˆ™åº“ï¼Œè¯¥è§„åˆ™åº“æºäºå¤§é‡ç¡¬ä»¶æµ‹è¯•ä¸åœºæ™¯éªŒè¯ï¼Œæ¶µç›–ä»¥ä¸‹æ ¸å¿ƒç»´åº¦çš„é€‚é…è§„åˆ™ï¼š
- æ¡ˆä¾‹ï¼šå¯å‘å¼è°ƒæ•´ block size

æ¡ˆä¾‹ 1ï¼šå‘é‡åŠ æ³• - æ ¹æ®æ•°æ®è§„æ¨¡è°ƒæ•´

```
import triton
import triton.language as tl

def heuristic_add_block_size(args):
    """æ ¹æ®è¾“å…¥æ•°æ®é‡åŠ¨æ€é€‰æ‹©æœ€ä¼˜ block size"""
    n_elements = args['n_elements']

    # å¯å‘å¼è§„åˆ™ï¼š
    # - æ•°æ®é‡ < 1024ï¼šä½¿ç”¨å° blockï¼Œå‡å°‘æ•°æ®å¡«å……å¼€é”€
    # - æ•°æ®é‡ >= 1024ï¼šä½¿ç”¨å¤§ blockï¼Œæå‡å†…å­˜åŠ è½½æ•ˆç‡
    if n_elements < 1024:
        return {'BLOCK_SIZE': 128}
    elif n_elements < 65536:
        return {'BLOCK_SIZE': 256}
    elif n_elements < 262144:
        return {'BLOCK_SIZE': 512}
    else:
        return {'BLOCK_SIZE': 1024}

@triton.heuristics(values={'BLOCK_SIZE': heuristic_add_block_size})
@triton.jit
def add_kernel(
    x_ptr, y_ptr, output_ptr, n_elements,
    BLOCK_SIZE: tl.constexpr
):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    tl.store(output_ptr + offsets, output, mask=mask)
```

æ¡ˆä¾‹ 2ï¼šçŸ©é˜µä¹˜æ³• - å¤šå‚æ•°å¯å‘å¼ä¼˜åŒ–

```
import math

def next_power_of_2(n):
    """è¿”å›å¤§äºç­‰äºnçš„æœ€å°2çš„å¹‚"""
    return1 << (n - 1).bit_length() if n > 1else1

def heuristic_matmul_block_size(args):
    """çŸ©é˜µä¹˜æ³•çš„å¤šå‚æ•°å¯å‘å¼ä¼˜åŒ–"""
    M, N, K = args['M'], args['N'], args['K']

    # æ ¹æ®çŸ©é˜µç»´åº¦é€‰æ‹©åˆ†å—å¤§å°
    # è§„åˆ™ï¼šé€‰æ‹©2çš„å¹‚æ¬¡ï¼Œç¡®ä¿å†…å­˜å¯¹é½å’Œåˆå¹¶è®¿é—®
    return {
        'BLOCK_M': min(128, next_power_of_2(M // 16)),
        'BLOCK_N': min(256, next_power_of_2(N // 8)),
        'BLOCK_K': min(64, next_power_of_2(K // 32))
    }

def heuristic_matmul_tiling(args):
    """æ ¹æ®è®¡ç®—å¼ºåº¦é€‰æ‹©åˆ†å—ç­–ç•¥"""
    M, N, K = args['M'], args['N'], args['K']
    total_ops = 2 * M * N * K
    total_bytes = 4 * (M * K + K * N + M * N)  # float32
    arithmetic_intensity = total_ops / total_bytes

    # æ ¹æ®ç®—æœ¯å¼ºåº¦è°ƒæ•´ç­–ç•¥
    if arithmetic_intensity > 20:  # è®¡ç®—å¯†é›†å‹
        return {'GROUP_SIZE_M': 1}  # å‡å°‘åˆ†ç»„ï¼Œæé«˜è®¡ç®—å±€éƒ¨æ€§
    else:  # å†…å­˜å¯†é›†å‹
        return {'GROUP_SIZE_M': 8}  # å¢åŠ åˆ†ç»„ï¼Œæ”¹å–„å†…å­˜è®¿é—®

# ç»„åˆå¤šä¸ªå¯å‘å¼å‡½æ•°
@triton.heuristics(values={
    'BLOCK_M': lambda args: heuristic_matmul_block_size(args)['BLOCK_M'],
    'BLOCK_N': lambda args: heuristic_matmul_block_size(args)['BLOCK_N'],
    'BLOCK_K': lambda args: heuristic_matmul_block_size(args)['BLOCK_K'],
    'GROUP_SIZE_M': heuristic_matmul_tiling,
})
@triton.jit
def matmul_kernel(
    a_ptr, b_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # çŸ©é˜µä¹˜æ³•æ ¸å¿ƒé€»è¾‘
    pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_M)
    num_pid_n = tl.cdiv(N, BLOCK_N)
    num_pid_in_group = GROUP_SIZE_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_SIZE_M
    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
    pid_m = first_pid_m + (pid % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m

    # çŸ©é˜µåˆ†å—è®¡ç®—
    # ... è¯¦ç»†å®ç°çœç•¥ ...
```

### è°ƒä¼˜å·¥å…· Proton

Proton æ˜¯ Triton å®˜æ–¹æ¨å‡ºçš„ GPU æ€§èƒ½åˆ†æå·¥å…·ï¼ŒåŸºäº NVIDIA NSight å¼€å‘ï¼Œæ”¯æŒç²¾å‡†ç›‘æ§ç¡¬ä»¶èµ„æºä½¿ç”¨æƒ…å†µï¼ˆå¦‚ FLOPsã€æ˜¾å­˜å¸¦å®½ã€çº¿ç¨‹åˆ©ç”¨ç‡ï¼‰ï¼Œç”Ÿæˆå¯è§†åŒ–æ€§èƒ½æŠ¥å‘Šï¼Œå¸®åŠ©å¼€å‘è€…å¿«é€Ÿå®šä½æ€§èƒ½ç“¶é¢ˆï¼ˆå¦‚æ¡†æ¶å¼€é”€è¿‡å¤§ã€å†…å­˜è®¿é—®æ•ˆç‡ä½ï¼‰ã€‚

- æ ¸å¿ƒåŠŸèƒ½
- åŸºæœ¬ä½¿ç”¨æµç¨‹

å®‰è£…ä¾èµ–ï¼š

```
# ç¼–è¯‘å®‰è£… Triton å¹¶å¯ç”¨ Proton æ¨¡å—
TRITON_BUILD_PROTON=ON pip install -e .

# å®‰è£…å¯è§†åŒ–ä¾èµ– hatchet
pip install llnl-hatchet
```

å¯¹æŒ‡å®šåŒºåŸŸåˆ†æï¼š

```
import torch
import triton
import triton.language as tl
import triton.profiler as proton

# å®šä¹‰å¾…åˆ†æçš„ add ç®—å­ï¼ˆçœç•¥ kernel å®ç°ï¼Œå¤ç”¨å‰æ–‡ add_kernelï¼‰
@triton.jit
def add_kernel(...):
    ...

def add(x, y):
    n_elements = x.numel()
    output = torch.empty_like(x)
    BLOCK_SIZE = 1024
    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE)
    return output

# å¯åŠ¨æ€§èƒ½åˆ†æï¼ŒæŒ‡å®šåˆ†æåç§°ï¼Œä¿å­˜æŠ¥å‘Šåˆ° vec_add_analysis.hatchet
proton.start("vec_add_analysis", hook="triton")

# æµ‹è¯•ä»£ç ï¼ˆå¾…åˆ†æåŒºåŸŸï¼‰
DEVICE = "cuda"if torch.cuda.is_available() else"cpu"
torch.manual_seed(0)
size = 98432
x = torch.rand(size, device=DEVICE)
y = torch.rand(size, device=DEVICE)
output_torch = x + y
output_triton = add(x, y)

# éªŒè¯ç»“æœæ­£ç¡®æ€§
print(f"æœ€å¤§å€¼å·®å¼‚: {torch.max(torch.abs(output_torch - output_triton))}")

# ç»“æŸåˆ†æå¹¶ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
proton.finalize()
```

å¯è§†åŒ–æŠ¥å‘Šï¼š

```
# å‘½ä»¤è¡ŒæŸ¥çœ‹vec_add_analysis.hatchetåŒ…å«æŒ‡æ ‡
# python -m triton.profiler.viewer --list vec_add_analysis.hatchet

# æ–¹æ³•ä¸€ï¼šå‘½ä»¤è¡Œå·¥å…·
# proton-viewer -m tflops/s_time/s vec_add_analysis.hatchet  # ä»¥TFLOPså’Œæ—¶é—´ä¸ºæŒ‡æ ‡

# æ–¹æ³•äºŒ: ä»£ç æ·»åŠ 
import triton.profiler.viewer as proton_viewer
metric_names = ["tflops/s", "time/ms"]
tree, metrics = proton_viewer.parse(metric_names, "vec_add_analysis.hatchet")
proton_viewer.print_tree(tree, metrics)
```

æŠ¥å‘Šåˆ†æï¼š

ä¸Šå›¾çš„ç»“æœä½¿ç”¨ time/ms ä½œä¸ºæ€§èƒ½æŒ‡æ ‡åˆ†æ vec_add çš„æ€§èƒ½æ¶ˆè€—ï¼Œæ˜¾ç¤ºäº†ä¸€ä¸ªæ ¹èŠ‚ç‚¹ `ROOT: 0.097 ms` å’Œå¤šä¸ªå­èŠ‚ç‚¹ï¼Œå³æ€»æ‰§è¡Œæ—¶é—´æ˜¯ 0.097 msï¼ˆæ ¹èŠ‚ç‚¹ï¼‰ï¼ŒTriton å†…æ ¸ `add_kernel` åªå äº† 0.002 msï¼ˆæœ€åä¸€è¡Œï¼‰ï¼Œå¤§éƒ¨åˆ†æ—¶é—´ï¼ˆ0.095 msï¼‰èŠ±åœ¨äº† PyTorch æ¡†æ¶çš„å¼€é”€ä¸Šå¦‚å†…å­˜åˆ†é…å’Œæ‹·è´ã€å¼ é‡åˆå§‹åŒ–ã€CUDA ä¸Šä¸‹æ–‡ç®¡ç†ä»¥åŠå…¶ä»–è¾…åŠ©æ“ä½œã€‚

å›¾ä¾‹æ˜¾ç¤ºäº†æ—¶é—´å æ¯”çš„é¢œè‰²ç¼–ç ï¼š

ğŸ”´ æ·±è‰² (0.09-0.10 ms): ROOT èŠ‚ç‚¹

ğŸŸ¡ ä¸­ç­‰è‰² (0.03-0.09 ms): å¤šä¸ª PyTorch å†…æ ¸

ğŸŸ¢ æµ…è‰² (0.00-0.03 ms): `add_kernel` å’Œå…¶ä»–å°å¼€é”€

- é«˜çº§ç”¨æ³•: è¯­ä¹‰æ ‡æ³¨ç”¨æˆ·åŒºåŸŸ

é€šè¿‡ proton.scope ä¸ºä¸åŒç®—å­å®ç°æ·»åŠ è¯­ä¹‰æ ‡ç­¾ï¼Œå¯ç›´è§‚å¯¹æ¯”æ€§èƒ½å·®å¼‚ï¼ˆå¦‚ cuBLASã€PyTorch åŸç”Ÿã€Triton å®ç°çš„çŸ©é˜µä¹˜æ³•æ€§èƒ½ï¼‰ï¼š

```
import cublas
import torch

def cublas_matmul(a, b):
    # æ ¡éªŒç»´åº¦å…¼å®¹æ€§
    assert a.shape[1] == b.shape[1], "Incompatible dimensions (b is transposed)"
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype
    c = torch.empty((M, N), device=a.device, dtype=dtype)
    bytes_per_elem = a.element_size()
    flops_str = f"flops{bytes_per_elem * 8}"# å®šä¹‰ FLOPs æŒ‡æ ‡ï¼ˆåŒºåˆ†ç²¾åº¦ï¼‰
    # æ·»åŠ è¯­ä¹‰æ ‡ç­¾ï¼Œè®°å½•æ€§èƒ½æŒ‡æ ‡
    with proton.scope(f"cublas [M={M}, N={N}, K={K}]",
                      {"bytes": bytes_per_elem * (M * K + N * K + M * N),
                       flops_str: 2. * M * N * K}):
        cublas.matmul(a, b, c)
    return c

def torch_matmul(a, b):
    M, K = a.shape
    N, K = b.shape
    bytes_per_elem = a.element_size()
    flops_str = f"flops{bytes_per_elem * 8}"
    with proton.scope(f"torch [M={M}, N={N}, K={K}]",
                      {"bytes": bytes_per_elem * (M * K + N * K + M * N),
                       flops_str: 2. * M * N * K}):
        c = torch.matmul(a, b.T)
    return c

# æ€§èƒ½åŸºå‡†æµ‹è¯•
def bench(K, dtype, reps=10000, warmup_reps=10000):
    M = 8192
    N = 8192
    a = torch.randn((M, K), device="cuda", dtype=torch.float16).to(dtype)
    b = torch.randn((K, N), device="cuda", dtype=torch.float16).to(dtype)
    b = b.T.contiguous()  # è½¬ç½®å¹¶ç¡®ä¿å†…å­˜è¿ç»­

    # æµ‹è¯•ä¸åŒå®ç°çš„æ€§èƒ½
    if cublas isnotNone:
        bench_fn("cublas", reps, warmup_reps, cublas_matmul, a, b)
    if dtype == torch.float16:
        bench_fn("torch", reps, warmup_reps, torch_matmul, a, b)
    # å¯æ·»åŠ  Triton å®ç°çš„æµ‹è¯•ä»£ç 
    # bench_fn("triton", reps, warmup_reps, triton_matmul, a, b)
```

ä¸Šå›¾æ˜¾ç¤ºäº†ä¸åŒçŸ©é˜µä¹˜æ³•å®ç°ï¼ˆåŒ…æ‹¬ Tritonï¼‰çš„æ€§èƒ½æ¯”è¾ƒï¼Œä½¿ç”¨ TFLOPS/sï¼ˆåŠç²¾åº¦ FLOPsï¼‰ ä½œä¸ºæ€§èƒ½æŒ‡æ ‡ã€‚åœ¨ 8192Ã—8192 çŸ©é˜µä¹˜æ³•åœºæ™¯ä¸‹ï¼ŒcuBLASã€PyTorch åŸç”Ÿã€Triton ä¸‰ç§å®ç°çš„æ€§èƒ½å‡åœ¨ 66-71 TFLOPS/s èŒƒå›´å†…ï¼Œæ€§èƒ½æ¥è¿‘ï¼Œè¯´æ˜ Triton ç®—å­å¯è¾¾åˆ°å·¥ä¸šçº§é«˜æ€§èƒ½æ°´å¹³ã€‚

## æ€»ç»“

æœ¬æ–‡ç³»ç»Ÿè®²è§£äº† Triton ç®—å­å¼€å‘ä¸­çš„è°ƒè¯•ä¸æ€§èƒ½ä¼˜åŒ–æ–¹æ³•ï¼š

1. è°ƒè¯•å±‚é¢ï¼šé€šè¿‡`static_print`/`device_print`å¿«é€Ÿæ’æŸ¥ç¼–è¯‘æ—¶ / è¿è¡Œæ—¶é”™è¯¯ï¼Œåˆ©ç”¨`interpreter`æ¨¡å¼ç»“åˆ pdb å®šä½é€»è¾‘æ¼æ´ï¼Œç¬¬ä¸‰æ–¹å·¥å…·è¾…åŠ©è§£å†³å¤æ‚å†…å­˜é—®é¢˜ï¼›
2. ä¼˜åŒ–å±‚é¢ï¼šAutotune è‡ªåŠ¨æœç´¢æœ€ä¼˜å‚æ•°ç»„åˆï¼Œå¯å‘å¼æœºåˆ¶é€‚é…ç¡¬ä»¶ä¸æ•°æ®è§„æ¨¡ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒæ•´ï¼›
3. æ€§èƒ½åˆ†æï¼šProton å·¥å…·ç²¾å‡†å®šä½ç“¶é¢ˆï¼Œé€šè¿‡å¯è§†åŒ–æŠ¥å‘ŠæŒ‡å¯¼è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚

æŒæ¡è¿™äº›æŠ€å·§åï¼Œå¼€å‘è€…å¯å¤§å¹…æå‡ Triton ç®—å­çš„å¼€å‘æ•ˆç‡ï¼Œå®ç°å…¼é¡¾æ­£ç¡®æ€§ä¸é«˜æ€§èƒ½çš„ç®—å­å®ç°ã€‚å®é™…å¼€å‘ä¸­ï¼Œå»ºè®®éµå¾ª â€œå…ˆè°ƒè¯•åä¼˜åŒ–â€ çš„æµç¨‹ï¼šå…ˆé€šè¿‡ interpreter æ¨¡å¼éªŒè¯é€»è¾‘æ­£ç¡®æ€§ï¼Œå†é€šè¿‡ Autotune ä¸å¯å‘å¼ä¼˜åŒ–æå‡æ€§èƒ½ï¼Œæœ€åç”¨ Proton å®šä½å‰©ä½™ç“¶é¢ˆï¼Œå®ç°æè‡´ä¼˜åŒ–ã€‚

--------END--------

ç‚¹å‡»é˜…è¯»åŸæ–‡ åŠ å…¥1nfinite
