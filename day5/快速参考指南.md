# MLP算子融合：快速参考指南

## 一、一句话总结

| 方案 | 总结 |
|------|------|
| **lite_llama** | silu + mul 融合，**代码简单易维护**，适合快速开发 |
| **triton_mlp** | matmul + silu 融合，**性能更优**，适合生产环境 |

---

## 二、快速对比表

```
┌─────────────────┬──────────────────┬──────────────────┐
│     维度        │   lite_llama     │   triton_mlp     │
├─────────────────┼──────────────────┼──────────────────┤
│ 代码复杂度      │ ⭐ 简单          │ ⭐⭐⭐ 复杂      │
│ 维护成本        │ ⭐ 低            │ ⭐⭐⭐ 高        │
│ 性能            │ ⭐⭐ 中等        │ ⭐⭐⭐ 优秀      │
│ 内存带宽        │ 4MN              │ 2MN              │
│ 数值精度        │ ⭐⭐⭐ 高        │ ⭐⭐ 中等        │
│ 学习曲线        │ ⭐ 平缓          │ ⭐⭐⭐ 陡峭      │
│ 推荐场景        │ 快速开发         │ 性能优先         │
└─────────────────┴──────────────────┴──────────────────┘
```

---

## 三、核心差异

### 3.1 融合点不同

```
lite_llama:
y1 = matmul(x, w1)
y2 = matmul(x, w2)
out = [silu(y1) * y2]  ← 融合点：向量级别
     ↑
  在这里融合

triton_mlp:
w1x = [matmul(x, w1) + silu]  ← 融合点：matmul级别
      ↑
   在这里融合
w3x = matmul(x, w3)
mul_out = w1x * w3x
```

### 3.2 内存访问不同

```
lite_llama 的内存访问：
y1 → 内存 → swiglu → 内存
y2 → 内存 ↗
总带宽：4MN

triton_mlp 的内存访问：
w1x 在寄存器中做silu，避免额外内存访问
总带宽：2MN（节省50%）
```

### 3.3 代码复杂度不同

```
lite_llama kernel（~20行）：
@triton.jit
def _swiglu_forward_kernel(a_ptr, b_ptr, c_ptr, ...):
    a_row = tl.load(a_ptr + col_offsets).to(tl.float32)
    b_row = tl.load(b_ptr + col_offsets)
    c_row = silu(a_row) * b_row
    tl.store(c_ptr + col_offsets, c_row)

triton_mlp kernel（~50行）：
@triton.jit
def matmul_kernel(..., need_silu, ...):
    # 复杂的二维网格映射
    # 复杂的指针计算
    # 循环中的dot product
    # 条件融合silu
    # 复杂的存储逻辑
```

---

## 四、选择决策树

```
┌─ 性能要求高？
│  ├─ 是 → 计算密度低（推理）？
│  │      ├─ 是 → 选择 triton_mlp ✓
│  │      └─ 否 → 两者都可
│  └─ 否 → 选择 lite_llama ✓
│
└─ 代码可维护性优先？
   ├─ 是 → 选择 lite_llama ✓
   └─ 否 → 选择 triton_mlp ✓
```

---

## 五、性能数据速查

### 5.1 内存带宽节省

```
假设：M=4, N=2048, K=4096

lite_llama:
  内存访问 = 4MN = 4 × 4 × 2048 × 2 = 65,536 字节

triton_mlp:
  内存访问 = 2MN = 2 × 4 × 2048 × 2 = 32,768 字节

节省比例 = (65,536 - 32,768) / 65,536 = 50%
```

### 5.2 时间节省估算

```
A100 GPU 内存带宽：2TB/s

lite_llama 时间：65,536 / (2TB/s) ≈ 0.03ms
triton_mlp 时间：32,768 / (2TB/s) ≈ 0.015ms

时间节省：0.015ms（对于单次推理可能不明显，但累积效果显著）
```

---

## 六、实现要点速查

### 6.1 lite_llama 实现要点

```python
# 1. 一维kernel，每个program处理一行
program_id = tl.program_id(0)

# 2. 加载两个向量
a_row = tl.load(a_ptr + col_offsets).to(tl.float32)
b_row = tl.load(b_ptr + col_offsets)

# 3. 融合silu + mul
c_row = silu(a_row) * b_row

# 4. 存储结果
tl.store(c_ptr + col_offsets, c_row)
```

### 6.2 triton_mlp 实现要点

```python
# 1. 二维kernel，复杂的网格映射
pid = tl.program_id(axis=0)
pid_m, pid_n = 复杂的映射逻辑

# 2. 循环中的dot product
accumulator = tl.zeros(..., dtype=tl.float32)
for k in range(...):
    accumulator = tl.dot(a, b, accumulator)

# 3. 在accumulator中融合silu
if need_silu:
    sigmoid_x = 1. / (1. + tl.exp(-accumulator))
    c = accumulator * sigmoid_x

# 4. 存储结果
tl.store(c_ptrs, c)
```

---

## 七、常见问题

### Q1: 为什么triton_mlp性能更好？
**A:** 因为silu计算在matmul的fp32 accumulator中进行，避免了中间结果的全局内存访问，节省了50%的内存带宽。

### Q2: 为什么不总是用triton_mlp？
**A:** 因为代码复杂度高，维护成本大。当计算密度足够高时（如训练），内存带宽不是瓶颈，复杂度不值得。

### Q3: 两者的精度有区别吗？
**A:** lite_llama在fp32中计算silu，精度更高。triton_mlp在fp32中计算但转换为fp16存储，精度略低但性能更优。

### Q4: 什么时候应该用triton_mlp？
**A:** 推理延迟敏感、计算密度低（batch_size小）的场景。

### Q5: 什么时候应该用lite_llama？
**A:** 代码可维护性优先、性能要求不极致的场景。

---

## 八、学习路径

```
第1天：理解MLP的基本结构
  └─ 学习标准PyTorch实现

第2天：学习lite_llama的融合方案
  └─ 理解向量级别的融合
  └─ 学习简单的Triton kernel

第3天：学习triton_mlp的融合方案
  └─ 理解matmul级别的融合
  └─ 学习复杂的Triton kernel

第4天：性能对比和优化
  └─ 使用nsys进行性能分析
  └─ 理解内存带宽和计算密度

第5天：实践和总结
  └─ 根据实际需求选择方案
  └─ 自己实现优化版本
```

---

## 九、关键代码片段

### lite_llama 的融合点
```python
# 在这一行融合了silu和mul
c_row = silu(a_row) * b_row
```

### triton_mlp 的融合点
```python
# 在这里融合了matmul和silu
if need_silu:
    sigmoid_x = 1. / (1. + tl.exp(-accumulator))
    c = accumulator.to(tl.float16) * sigmoid_x.to(tl.float16)
```

---

## 十、总结

| 特性 | lite_llama | triton_mlp |
|------|-----------|-----------|
| **适合初学者** | ✓ | ✗ |
| **适合生产环境** | ✓ | ✓✓ |
| **代码行数** | ~20 | ~50 |
| **性能提升** | 基础 | 25-50% |
| **学习成本** | 低 | 高 |

**最终建议：** 先学lite_llama理解基础，再学triton_mlp追求极致性能。

