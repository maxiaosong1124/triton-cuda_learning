# MLP算子融合：代码实现细节对比

## 一、lite_llama 实现详解

### 1.1 swiglu_forward 的实现

```python
@triton.jit
def _swiglu_forward_kernel(
    a_ptr, b_ptr, c_ptr, row_stride, 
    n_cols: tl.constexpr, BLOCK_SIZE: tl.constexpr
):
    program_id = tl.program_id(0).to(tl.int64)
    
    # 定位到当前行
    a_ptr += program_id * row_stride
    b_ptr += program_id * row_stride
    c_ptr += program_id * row_stride
    
    col_offsets = tl.arange(0, BLOCK_SIZE)
    mask = col_offsets < n_cols
    
    # 关键：silu在fp32中计算，精度高
    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0).to(tl.float32)
    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)
    
    # 融合点：silu(a) * b
    c_row = silu(a_row) * b_row
    tl.store(c_ptr + col_offsets, c_row, mask=mask)
```

**特点分析：**
- **一维kernel**：每个program处理一行数据
- **简单高效**：只需加载两个向量，计算silu + mul
- **精度优先**：a_row转换为fp32计算sigmoid
- **内存访问**：顺序访问，缓存友好

### 1.2 完整MLP流程

```python
def torch_mlp_silu(x, w1, w2, w3):
    batch, seq_len, dim = x.shape
    M, K = batch * seq_len, dim
    x = x.view(M, K)
    
    # 第1步：两个独立的matmul
    y1 = torch.mm(x, w1)  # (M, N)
    y2 = torch.mm(x, w2)  # (M, N)
    
    # 第2步：融合silu + mul
    out = swiglu_forward(y1, y2)  # (M, N)
    
    # 第3步：最后的matmul
    mlp_out = torch.mm(out, w3)  # (M, K)
    mlp_out = mlp_out.view(batch, seq_len, -1)
    return mlp_out
```

**数据流：**
```
x (M×K)
├─→ matmul(w1) → y1 (M×N) ─┐
└─→ matmul(w2) → y2 (M×N) ─┼→ swiglu → out (M×N) → matmul(w3) → output
```

---

## 二、triton_mlp 实现详解

### 2.1 matmul_kernel 的实现

```python
@triton.jit
def matmul_kernel(
    a_ptr, b_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    need_silu,  # 关键参数：是否需要silu
    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, 
    BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr,
):
    # 二维网格映射
    pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    num_pid_in_group = GROUP_SIZE_M * num_pid_n
    group_id = pid // num_pid_in_group
    first_pid_m = group_id * GROUP_SIZE_M
    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)
    pid_n = (pid % num_pid_in_group) // group_size_m
    
    # 计算指针偏移
    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)
    
    # 累加计算（fp32）
    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, b, accumulator)
        a_ptrs += BLOCK_SIZE_K * stride_ak
        b_ptrs += BLOCK_SIZE_K * stride_bk
    
    # 融合点：在accumulator中计算silu
    if need_silu:
        sigmoid_x = 1. / (1. + tl.exp(-accumulator))
        c = accumulator.to(tl.float16) * sigmoid_x.to(tl.float16)
    else:
        c = accumulator.to(tl.float16)
    
    # 存储结果
    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
    tl.store(c_ptrs, c, mask=c_mask)
```

**关键特点：**
- **二维kernel**：更好的并行性和缓存利用
- **条件融合**：通过need_silu参数控制是否融合silu
- **寄存器优化**：silu在fp32 accumulator中计算，避免内存访问
- **复杂的网格映射**：GROUP_SIZE_M优化缓存局部性

### 2.2 完整MLP流程

```python
def mlp(inputs, w1t, w2t, w3t):
    # 第1步：matmul + silu融合
    w1x = matmul(inputs, w1t, need_silu=True)
    
    # 第2步：普通matmul
    w3x = matmul(inputs, w3t, need_silu=False)
    
    # 第3步：向量乘法
    mul_out = vector_mul(w1x, w3x)
    
    # 第4步：最后的matmul
    out = matmul(mul_out, w2t, need_silu=False)
    return out
```

**数据流：**
```
inputs (M×K)
├─→ matmul+silu(w1) → w1x (M×N) ─┐
└─→ matmul(w3) → w3x (M×N) ──────┼→ vector_mul → mul_out → matmul(w2) → output
```

---

## 三、关键差异对比

### 3.1 kernel复杂度

| 方面 | lite_llama | triton_mlp |
|------|-----------|-----------|
| **kernel维度** | 1D | 2D |
| **网格映射** | 简单 | 复杂（GROUP_SIZE_M） |
| **指针计算** | 简单 | 复杂 |
| **融合逻辑** | 简单（silu+mul） | 复杂（在accumulator中） |
| **代码行数** | ~20行 | ~50行 |

### 3.2 内存访问模式

**lite_llama：**
```
Load: a_row (N元素) + b_row (N元素)
Compute: silu(a_row) * b_row
Store: c_row (N元素)
总访问：3N元素
```

**triton_mlp：**
```
Load: a (BLOCK_SIZE_M × BLOCK_SIZE_K) + b (BLOCK_SIZE_K × BLOCK_SIZE_N)
Compute: dot product + silu（在寄存器中）
Store: c (BLOCK_SIZE_M × BLOCK_SIZE_N)
总访问：BLOCK_SIZE_M × BLOCK_SIZE_N + BLOCK_SIZE_K × (BLOCK_SIZE_M + BLOCK_SIZE_N)
```

### 3.3 精度与性能权衡

**lite_llama：**
```python
a_row = tl.load(...).to(tl.float32)  # 转fp32
c_row = silu(a_row) * b_row          # fp32计算
# 精度：高，但性能稍低
```

**triton_mlp：**
```python
accumulator = tl.zeros(..., dtype=tl.float32)  # fp32累加
if need_silu:
    sigmoid_x = 1. / (1. + tl.exp(-accumulator))
    c = accumulator.to(tl.float16) * sigmoid_x.to(tl.float16)
# 精度：中等，但性能更优
```

---

## 四、学习建议

### 4.1 理解顺序

1. **第一步**：理解lite_llama的简单融合
   - 学习基本的Triton kernel编写
   - 理解向量级别的融合概念

2. **第二步**：理解triton_mlp的复杂融合
   - 学习二维kernel和网格映射
   - 理解寄存器级别的优化

3. **第三步**：性能对比和优化
   - 使用nsys/ncu进行性能分析
   - 理解内存带宽和计算密度的关系

### 4.2 实践建议

```python
# 建议1：从lite_llama开始
# 简单易懂，快速上手

# 建议2：逐步优化到triton_mlp
# 理解每一步的优化点

# 建议3：根据实际需求选择
# 不是所有场景都需要triton_mlp的复杂度
```

