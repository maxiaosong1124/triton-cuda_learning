# Triton 算子实现模式总结

## 概述

在学习 Triton 编程过程中，发现不同类型的算子在实现时采用不同的数据处理策略。主要分为两大类：

1. **Element-wise 类型算子**：逐元素操作，将多维矩阵展开为一维处理
2. **归约类型算子**：以行为单位进行归约操作，保持二维结构处理

---

## 1. Element-wise 类型算子

### 核心思想

Element-wise 算子对每个元素独立进行相同的操作，元素之间没有依赖关系。为了简化实现和提高并行性，**将二维（或多维）矩阵展开为一维连续的元素序列**进行处理。

### 关键特点

- 使用 `numel()` 获取张量总元素数
- 将数据视为一维连续数组
- 每个线程块处理一段连续的元素
- 通过全局偏移量 `pid * BLOCK_SIZE + offsets` 访问数据
- 不需要关心原始的维度结构

### 实现示例：向量加法

```python
import torch
import triton
import triton.language as tl

@triton.jit
def add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    # 获取当前线程块的 ID
    pid = tl.program_id(axis=0)

    # 计算当前块处理的起始位置（一维展开）
    block_start = pid * BLOCK_SIZE

    # 计算当前块内每个线程的全局偏移量
    offsets = block_start + tl.arange(0, BLOCK_SIZE)

    # 边界检查：避免越界访问
    mask = offsets < n_elements

    # 加载数据（一维偏移）
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)

    # 逐元素计算
    output = x + y

    # 存储结果（一维偏移）
    tl.store(output_ptr + offsets, output, mask=mask)

def add(x, y):
    output = torch.empty_like(x)

    # 关键：将多维张量展开为一维，获取总元素数
    n_elements = output.numel()

    # 启动足够多的线程块来覆盖所有元素
    grids = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)

    add_kernel[grids](x, y, output, n_elements, BLOCK_SIZE=1024)
    return output
```

### 适用场景

- 加法、减法、乘法、除法等二元操作
- ReLU、sigmoid、tanh 等激活函数
- 元素级别的数学运算（abs, sqrt, exp, log 等）
- 所有不需要跨元素通信的操作

---

## 2. 归约类型算子

### 核心思想

归约算子需要沿着某个维度对数据进行聚合操作（如求和、求最大值、求平均值）。这类操作需要**以行（或其他维度）为单位进行处理**，每个线程块负责处理一行或多行数据，在该维度内进行归约计算。

### 关键特点

- 保持二维（或多维）结构
- 每个线程块处理一行或多行
- 使用 `row_idx * n_cols + col_offsets` 访问二维数据
- 需要进行维度内的归约操作（sum, max, mean 等）
- 通常沿着最后一个维度（列）进行归约

### 实现示例 1：RMSNorm

```python
import torch
import triton
import triton.language as tl

@triton.jit
def rmsnorm_kernel(x_ptr, gamma_ptr, output_ptr, n_rows, n_cols,
                   eps: tl.constexpr, BLOCK_SIZE: tl.constexpr):
    # 获取当前线程块负责的行索引
    row_idx = tl.program_id(axis=0)

    if row_idx >= n_rows:
        return

    # 计算当前行的起始指针（二维结构）
    row_start_ptr = x_ptr + row_idx * n_cols

    # 计算列偏移量
    offsets = tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_cols

    # 加载当前行的数据
    x = tl.load(row_start_ptr + offsets, mask=mask, other=0.0)

    # 归约操作：计算平方和的均值
    mean = tl.sum(x * x, axis=0) / n_cols
    rms = tl.sqrt(mean + eps)

    # 归一化
    x = x / rms

    # 加载缩放参数
    gamma = tl.load(gamma_ptr + offsets, mask=mask, other=0.0)
    output = x * gamma

    # 存储结果到对应的行
    tl.store(output_ptr + row_idx * n_cols + offsets, output, mask=mask)

def rmsnorm(x, gamma, eps=1e-5):
    output = torch.empty_like(x)
    n_rows, n_cols = x.shape

    # 每个线程块处理一行
    grids = lambda meta: (n_rows,)

    rmsnorm_kernel[grids](x, gamma, output, n_rows, n_cols, eps, BLOCK_SIZE=1024)
    return output
```

### 实现示例 2：Softmax

```python
import torch
import triton
import triton.language as tl

@triton.jit
def softmax_kernel(input_ptr, output_ptr, n_rows, n_cols,
                   BLOCK_SIZE: tl.constexpr, input_row_stride, output_row_stride):
    # 获取当前线程块 ID
    pid = tl.program_id(0)

    # 每个线程块处理多行（这里处理 2 行）
    row_len = 2
    row_start = pid * row_len

    if row_start >= n_rows:
        return

    # 循环处理分配给当前块的多行
    for row_idx in tl.range(row_start, row_start + row_len, 1):
        # 计算当前行的起始指针
        row_start_ptr = input_ptr + row_idx * input_row_stride

        # 列偏移量
        col_offsets = tl.arange(0, BLOCK_SIZE)
        mask = col_offsets < n_cols

        # 计算当前行的数据指针
        input_ptrs = row_start_ptr + col_offsets

        # 加载当前行
        row = tl.load(input_ptrs, mask=mask, other=-float('inf'))

        # 归约操作 1：找到最大值（数值稳定性）
        row_max = tl.max(row, axis=0)

        # 计算 exp(x - max)
        numerator = tl.exp(row - row_max)

        # 归约操作 2：求和
        denominator = tl.sum(numerator, axis=0)

        # 归一化
        softmax_output = numerator / denominator

        # 计算输出行的起始指针
        output_row_start_ptr = output_ptr + row_idx * output_row_stride
        output_ptrs = output_row_start_ptr + col_offsets

        # 存储结果
        tl.store(output_ptrs, softmax_output, mask=mask)
```

### 适用场景

- Softmax、LayerNorm、RMSNorm、BatchNorm
- 矩阵的行/列求和、求均值
- Attention 机制中的归一化操作
- 所有需要跨元素通信和聚合的操作

---

## 两种模式的对比总结

| 特性 | Element-wise 类型 | 归约类型 |
|------|------------------|----------|
| **数据处理方式** | 一维展开 | 保持二维结构 |
| **元素依赖关系** | 无依赖，完全独立 | 同一行/列内元素相互依赖 |
| **偏移量计算** | `pid * BLOCK_SIZE + offsets` | `row_idx * n_cols + col_offsets` |
| **grid 维度** | `(cdiv(n_elements, BLOCK_SIZE),)` | `(n_rows,)` 或更复杂的划分 |
| **归约操作** | 无 | 有（sum, max, mean 等） |
| **示例算子** | add, multiply, relu, sigmoid | softmax, layernorm, rmsnorm |
| **并行粒度** | 元素级 | 行级或块级 |
| **内存访问模式** | 连续访问 | 行内连续，行间跳跃 |

---

## 关键经验总结

1. **选择正确的数据布局**：
   - 如果操作是逐元素独立的 → 展开为一维
   - 如果需要跨维度聚合 → 保持原始维度结构

2. **Grid 和 Block 的设计**：
   - Element-wise：grid 大小由总元素数决定
   - 归约类型：grid 大小由归约维度之外的维度决定（如行数）

3. **内存访问优化**：
   - 两种模式都应该保证连续的内存访问
   - Element-wise 天然连续
   - 归约类型需要确保行内访问连续

4. **边界处理**：
   - 使用 mask 处理不能被 BLOCK_SIZE 整除的情况
   - Element-wise：`offsets < n_elements`
   - 归约类型：`col_offsets < n_cols` 和 `row_idx < n_rows`

---

## 学习日期

- Day 2: Element-wise 算子实践（add, multiply, sub）
- Day 3: 归约类型算子实践（softmax, rms_norm）
